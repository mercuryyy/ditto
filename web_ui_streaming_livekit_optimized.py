"""
LiveKit Optimized Streaming Web UI for Ditto Talking Head using RunPod Serverless Endpoint
Optimized for RTX 4090 real-time avatar generation with ElevenLabs TTS integration
"""
import os
import base64
import requests
import gradio as gr
from PIL import Image
import io
import tempfile
import time
import threading
import json


class DittoLiveKitWebUI:
    def __init__(self, runpod_endpoint, runpod_api_key):
        """
        Initialize the LiveKit Optimized Streaming Web UI
        """
        self.runpod_endpoint = runpod_endpoint
        self.headers = {
            "Authorization": f"Bearer {runpod_api_key}",
            "Content-Type": "application/json"
        }
    
    def encode_image_to_base64(self, image):
        """Convert PIL Image to base64 string"""
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()
    
    def generate_elevenlabs_tts(self, text, api_key, voice_id):
        """Generate TTS audio using ElevenLabs API"""
        if not api_key or not voice_id:
            gr.Warning("Please provide ElevenLabs API key and voice ID")
            return None
            
        try:
            url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
            headers = {
                "Accept": "audio/mpeg",
                "Content-Type": "application/json",
                "xi-api-key": api_key
            }
            data = {
                "text": text,
                "model_id": "eleven_monolingual_v1",
                "voice_settings": {
                    "stability": 0.5,
                    "similarity_boost": 0.5
                }
            }
            
            gr.Info("üé§ Generating TTS with ElevenLabs...")
            response = requests.post(url, json=data, headers=headers)
            
            if response.status_code == 200:
                # Save audio to temporary file
                with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmp_file:
                    tmp_file.write(response.content)
                    gr.Info("‚úÖ TTS generated successfully!")
                    return tmp_file.name
            else:
                gr.Error(f"ElevenLabs API Error: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            gr.Error(f"TTS Generation Error: {str(e)}")
            return None
    
    def process_video_streaming(self, text, image, use_pytorch, enable_streaming, 
                              max_size, crop_scale, crop_vx_ratio, crop_vy_ratio,
                              emo, mouth_amplitude, head_amplitude, eye_amplitude,
                              sampling_timesteps, smo_k_s, smo_k_d, overlap_v2,
                              crop_flag_do_rot, relative_d, vad_alpha, delta_exp, flag_stitching):
        """
        Process video with LiveKit optimized settings for RTX 4090
        """
        if not text:
            gr.Warning("Please enter some text to synthesize.")
            return None, "Please enter some text to synthesize."
        
        if image is None:
            gr.Warning("Please upload an image.")
            return None, "Please upload an image."
        
        try:
            # Prepare the request payload
            image_base64 = self.encode_image_to_base64(image)
            
            # LiveKit optimized Ditto settings
            ditto_settings = {
                # Image/Avatar settings - optimized for real-time
                "max_size": int(max_size),  # 1280 for RTX 4090 balance
                "crop_scale": float(crop_scale),
                "crop_vx_ratio": float(crop_vx_ratio),
                "crop_vy_ratio": float(crop_vy_ratio),
                "crop_flag_do_rot": bool(crop_flag_do_rot),
                "template_n_frames": -1,  # Auto for LiveKit
                
                # Motion/Animation settings - optimized for natural movement
                "emo": int(emo),  # Reduced for more natural expression
                "sampling_timesteps": int(sampling_timesteps),  # Reduced for speed
                "smo_k_s": int(smo_k_s),  # Reduced for low latency
                "smo_k_d": int(smo_k_d),  # Reduced for responsiveness
                "relative_d": bool(relative_d),
                "eye_f0_mode": False,  # Not needed for LiveKit
                
                # Advanced settings - optimized for streaming
                "online_mode": True,  # Force online for LiveKit
                "overlap_v2": int(overlap_v2),  # Minimal overlap for low latency
                "delta_eye_open_n": 0,  # Default
                "fade_type": "",  # No fade for real-time
                
                # Fade settings - disabled for real-time
                "fade_in": -1,
                "fade_out": -1,
                
                # Amplitude controls - optimized for natural LiveKit avatars
                "mouth_amplitude": float(mouth_amplitude),  # Optimized
                "head_amplitude": float(head_amplitude),    # Reduced head movement
                "eye_amplitude": float(eye_amplitude),      # Natural eye movement
                
                # Advanced motion controls
                "vad_alpha": float(vad_alpha),
                "delta_pitch": 0.0,  # No manual pose for real-time
                "delta_yaw": 0.0,
                "delta_roll": 0.0,
                "alpha_pitch": 1.0,  # Default scaling
                "alpha_yaw": 1.0,
                "alpha_roll": 1.0,
                "delta_exp": float(delta_exp),
                "flag_stitching": bool(flag_stitching),
            }
            
            payload = {
                "input": {
                    "text": text,
                    "image_base64": image_base64,
                    "use_pytorch": use_pytorch,
                    "streaming_mode": enable_streaming,
                    "mode": "realtime",  # Use real-time streaming handler
                    "ditto_settings": ditto_settings
                }
            }
            
            # Send request to RunPod endpoint
            status_msg = "üöÄ Processing LiveKit real-time avatar..."
            gr.Info(status_msg)
            
            response = requests.post(
                f"{self.runpod_endpoint}/run",
                json=payload,
                headers=self.headers,
                timeout=180  # Reduced timeout for real-time use
            )
            
            if response.status_code != 200:
                error_msg = f"Failed to connect to RunPod endpoint. Status: {response.status_code}"
                gr.Error(error_msg)
                return None, error_msg
            
            result = response.json()
            
            # Handle real-time streaming response
            if "stream_results" in result and result.get("mode") == "realtime":
                # Real-time streaming mode response
                stream_results = result["stream_results"]
                frames_processed = sum(1 for r in stream_results if r.get("type") == "frame")
                
                status_msg = f"üé¨ Real-time avatar generated! ({frames_processed} frames streamed)"
                gr.Info(status_msg)
                
                # For now, return a placeholder since real-time streams don't have final video
                return None, status_msg
            
            # Check if the job was queued (async processing)
            if "id" in result:
                job_id = result["id"]
                status_msg = f"üìã Job queued with ID: {job_id}. Processing..."
                gr.Info(status_msg)
                return self.poll_job_status_streaming(job_id, enable_streaming)
            
            # Direct response (sync processing)
            if "output" in result:
                return self.process_output_streaming(result["output"], enable_streaming)
            
            error_msg = "Unexpected response format from RunPod"
            gr.Error(error_msg)
            return None, error_msg
            
        except requests.exceptions.Timeout:
            error_msg = "Request timed out. Consider reducing quality settings for faster processing."
            gr.Error(error_msg)
            return None, error_msg
        except Exception as e:
            error_msg = f"Error: {str(e)}"
            gr.Error(error_msg)
            return None, error_msg
    
    def poll_job_status_streaming(self, job_id, enable_streaming=True, max_attempts=36):
        """Poll RunPod for job completion with streaming updates"""
        
        last_progress = 0
        status_messages = []
        
        for attempt in range(max_attempts):
            try:
                response = requests.get(
                    f"{self.runpod_endpoint}/status/{job_id}",
                    headers=self.headers
                )
                
                if response.status_code != 200:
                    error_msg = f"Failed to check job status. Status: {response.status_code}"
                    gr.Error(error_msg)
                    return None, error_msg
                
                result = response.json()
                status = result.get("status")
                
                if status == "COMPLETED":
                    gr.Info("‚úÖ LiveKit avatar generated successfully!")
                    return self.process_output_streaming(result.get("output", {}), enable_streaming)
                    
                elif status == "FAILED":
                    error_msg = result.get("error", "Unknown error")
                    gr.Error(f"‚ùå Job failed: {error_msg}")
                    return None, f"Job failed: {error_msg}"
                    
                elif status in ["IN_QUEUE", "IN_PROGRESS"]:
                    # Show progress updates
                    if attempt % 2 == 0:  # Update every 10 seconds
                        status_msg = f"‚ö° Processing avatar... ({attempt+1}/{max_attempts})"
                        gr.Info(status_msg)
                    
                    time.sleep(5)  # Check every 5 seconds
                    
                else:
                    error_msg = f"Unknown job status: {status}"
                    gr.Error(error_msg)
                    return None, error_msg
                    
            except Exception as e:
                error_msg = f"Error checking job status: {str(e)}"
                gr.Error(error_msg)
                return None, error_msg
        
        error_msg = "Job timed out after 3 minutes"
        gr.Error(error_msg)
        return None, error_msg
    
    def process_output_streaming(self, output, enable_streaming=True):
        """Process the output with streaming information"""
        if "error" in output:
            error_msg = f"Error from RunPod: {output['error']}"
            gr.Error(error_msg)
            return None, error_msg
        
        if "video_base64" not in output:
            error_msg = "No video data in response"
            gr.Error(error_msg)
            return None, error_msg
        
        try:
            # Decode the video from base64
            video_data = base64.b64decode(output["video_base64"])
            
            # Save to temporary file
            with tempfile.NamedTemporaryFile(suffix=".mp4", delete=False) as tmp_file:
                tmp_file.write(video_data)
                
                status_msg = "üé¨ LiveKit avatar video ready!"
                gr.Info(status_msg)
                return tmp_file.name, status_msg
                
        except Exception as e:
            error_msg = f"Error processing video: {str(e)}"
            gr.Error(error_msg)
            return None, error_msg
    
    def launch(self):
        """Launch the LiveKit optimized Gradio interface"""
        with gr.Blocks(title="Ditto LiveKit Optimized Web UI", theme=gr.themes.Soft()) as interface:
            gr.Markdown(
                """
                # üöÄ Ditto LiveKit Real-Time Avatar Generator
                
                **Optimized for RTX 4090 + LiveKit Real-Time Avatars**
                
                Generate high-quality lip-synced talking head videos optimized for LiveKit agents with ElevenLabs TTS integration.
                
                ## ‚ö° RTX 4090 Optimizations:
                - **1280px Resolution**: Perfect balance for real-time processing
                - **25 Sampling Steps**: Optimal quality/speed ratio  
                - **Reduced Latency**: 3-frame overlap, fast smoothing
                - **Natural Movement**: Tuned amplitude controls for professional avatars
                
                ## üé§ ElevenLabs TTS Integration:
                - Generate high-quality voice synthesis
                - Stream audio alongside video generation
                - Professional voice options
                """
            )
            
            with gr.Row():
                with gr.Column(scale=1):
                    text_input = gr.Textbox(
                        label="Text to Synthesize",
                        placeholder="Enter the text for your LiveKit avatar to speak...",
                        lines=5
                    )
                    
                    # ElevenLabs TTS Section
                    with gr.Accordion("üé§ ElevenLabs TTS Integration", open=True):
                        with gr.Row():
                            elevenlabs_api_key = gr.Textbox(
                                label="ElevenLabs API Key",
                                placeholder="Enter your ElevenLabs API key...",
                                type="password"
                            )
                            elevenlabs_voice_id = gr.Textbox(
                                label="Voice ID",
                                placeholder="Enter ElevenLabs voice ID...",
                                value="21m00Tcm4TlvDq8ikWAM"  # Default Rachel voice
                            )
                        
                        generate_tts_btn = gr.Button("üéµ Generate TTS Audio", variant="secondary")
                        tts_audio_output = gr.Audio(
                            label="Generated TTS Audio (for preview)",
                            interactive=False
                        )
                    
                    image_input = gr.Image(
                        label="Upload Portrait Image",
                        type="pil"
                    )
                    
                    with gr.Row():
                        use_pytorch = gr.Checkbox(
                            label="Use PyTorch Model",
                            info="Use if TensorRT not available",
                            value=False
                        )
                        
                        enable_streaming = gr.Checkbox(
                            label="Enable Real-Time Mode",
                            info="Uses runpod_realtime_streaming_handler.py",
                            value=True
                        )
                    
                    # LiveKit Optimized Settings
                    with gr.Accordion("‚öôÔ∏è LiveKit Optimization Settings", open=False):
                        gr.Markdown("### RTX 4090 Optimized Settings")
                        with gr.Row():
                            max_size = gr.Slider(
                                minimum=512, maximum=2560, value=1280, step=128,
                                label="Max Resolution",
                                info="1280px optimized for RTX 4090 real-time"
                            )
                            sampling_timesteps = gr.Slider(
                                minimum=10, maximum=100, value=25, step=5,
                                label="Sampling Steps",
                                info="25 steps for optimal speed/quality"
                            )
                        
                        gr.Markdown("### Real-Time Movement Controls")
                        with gr.Row():
                            mouth_amplitude = gr.Slider(
                                minimum=0.0, maximum=2.0, value=0.8, step=0.1,
                                label="Mouth Movement",
                                info="Optimized for natural lip sync"
                            )
                            head_amplitude = gr.Slider(
                                minimum=0.0, maximum=2.0, value=0.6, step=0.1,
                                label="Head Movement",
                                info="Reduced for professional avatars"
                            )
                            eye_amplitude = gr.Slider(
                                minimum=0.0, maximum=2.0, value=0.9, step=0.1,
                                label="Eye Movement",
                                info="Natural eye expressions"
                            )
                        
                        gr.Markdown("### Advanced Real-Time Settings")
                        with gr.Row():
                            emo = gr.Slider(
                                minimum=0, maximum=7, value=3, step=1,
                                label="Emotion Level",
                                info="Reduced for professional appearance"
                            )
                            overlap_v2 = gr.Slider(
                                minimum=1, maximum=10, value=3, step=1,
                                label="Audio Overlap",
                                info="Minimal for low latency"
                            )
                        
                        with gr.Row():
                            smo_k_s = gr.Slider(
                                minimum=1, maximum=15, value=5, step=1,
                                label="Source Smoothing",
                                info="Fast smoothing for real-time"
                            )
                            smo_k_d = gr.Slider(
                                minimum=1, maximum=10, value=2, step=1,
                                label="Motion Smoothing",
                                info="Minimal for responsiveness"
                            )
                        
                        # Additional advanced controls
                        with gr.Accordion("üîß Expert Controls", open=False):
                            with gr.Row():
                                crop_scale = gr.Slider(
                                    minimum=1.0, maximum=5.0, value=2.3, step=0.1,
                                    label="Crop Scale"
                                )
                                crop_vx_ratio = gr.Slider(
                                    minimum=-0.5, maximum=0.5, value=0.0, step=0.01,
                                    label="Horizontal Offset"
                                )
                                crop_vy_ratio = gr.Slider(
                                    minimum=-0.5, maximum=0.5, value=-0.125, step=0.01,
                                    label="Vertical Offset"
                                )
                            
                            with gr.Row():
                                vad_alpha = gr.Slider(
                                    minimum=0.0, maximum=1.0, value=1.0, step=0.1,
                                    label="Voice Activity Level"
                                )
                                delta_exp = gr.Slider(
                                    minimum=-0.5, maximum=0.5, value=0.0, step=0.01,
                                    label="Expression Offset"
                                )
                            
                            with gr.Row():
                                flag_stitching = gr.Checkbox(
                                    label="Enable Stitching Network",
                                    value=True
                                )
                                crop_flag_do_rot = gr.Checkbox(
                                    label="Auto Rotation",
                                    value=True
                                )
                                relative_d = gr.Checkbox(
                                    label="Relative Motion",
                                    value=True
                                )
                    
                    generate_btn = gr.Button("üöÄ Generate LiveKit Avatar", variant="primary", size="lg")
                
                with gr.Column(scale=1):
                    status_display = gr.Textbox(
                        label="Status",
                        value="Ready for LiveKit avatar generation",
                        interactive=False,
                        lines=2
                    )
                    
                    video_output = gr.Video(
                        label="Generated Avatar Video"
                    )
            
            # LiveKit Information
            gr.Markdown(
                """
                ### üéØ LiveKit Integration Guide:
                
                **Optimized Settings for RTX 4090:**
                - **Resolution**: 1280px for optimal real-time performance
                - **Processing**: 25 sampling steps for 2-3x faster generation
                - **Latency**: Reduced audio overlap and smoothing for responsiveness
                - **Movement**: Tuned amplitudes for professional avatar appearance
                
                **Performance Expectations:**
                - **RTX 4090**: ~3-5 seconds per second of video (optimized)
                - **Memory Usage**: ~6-10GB VRAM at 1280px
                - **Quality**: Production-ready for professional applications
                
                **Real-Time Handler Active:**
                - Using `runpod_realtime_streaming_handler.py` for frame-by-frame processing
                - TTS removed from pipeline for maximum speed
                - ElevenLabs TTS handled separately for optimal quality
                
                ### üöÄ LiveKit Agents SDK Integration:
                1. Generate TTS audio using ElevenLabs integration above
                2. Generate avatar video with optimized settings below
                3. Use video as avatar base in LiveKit agents
                4. Stream TTS audio through LiveKit's audio pipeline
                5. Real-time lip sync handled by the generated avatar
                """
            )
            
            # Connect TTS generation
            generate_tts_btn.click(
                fn=self.generate_elevenlabs_tts,
                inputs=[text_input, elevenlabs_api_key, elevenlabs_voice_id],
                outputs=[tts_audio_output]
            )
            
            # Connect the generate button with properly mapped parameters
            generate_btn.click(
                fn=self.process_video_streaming,
                inputs=[
                    text_input, image_input, use_pytorch, enable_streaming,
                    max_size, crop_scale, crop_vx_ratio, crop_vy_ratio,
                    emo, mouth_amplitude, head_amplitude, eye_amplitude,
                    sampling_timesteps, smo_k_s, smo_k_d, overlap_v2,
                    crop_flag_do_rot, relative_d, vad_alpha, delta_exp, flag_stitching
                ],
                outputs=[video_output, status_display]
            )
        
        interface.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=True
        )


def main():
    """Main function to run the LiveKit optimized web UI"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Ditto LiveKit Optimized Web UI")
    parser.add_argument(
        "--runpod-endpoint",
        type=str,
        required=True,
        help="RunPod serverless endpoint URL (e.g., https://api.runpod.ai/v2/YOUR_ENDPOINT_ID)"
    )
    parser.add_argument(
        "--runpod-api-key",
        type=str,
        required=True,
        help="RunPod API key for authentication"
    )
    
    args = parser.parse_args()
    
    # Create and launch the LiveKit optimized web UI
    ui = DittoLiveKitWebUI(args.runpod_endpoint, args.runpod_api_key)
    ui.launch()


if __name__ == "__main__":
    # For testing, you can also set these as environment variables
    import os
    
    # Check if environment variables are set
    endpoint = os.getenv("RUNPOD_ENDPOINT")
    api_key = os.getenv("RUNPOD_API_KEY")
    
    if endpoint and api_key:
        ui = DittoLiveKitWebUI(endpoint, api_key)
        ui.launch()
    else:
        # Run with command line arguments
        main()
